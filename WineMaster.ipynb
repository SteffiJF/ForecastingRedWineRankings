{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WineMaster.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1_u6E9yOIbPXpTmo9VXrHfLcUIwnCeyP7",
      "authorship_tag": "ABX9TyNv2dx61AL7Uv9704k6ETd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteffiJF/ForecastingRedWineRankings/blob/main/WineMaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZKnqsjFcwc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sCh0OO8P7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a801d966-b3eb-4305-dad3-55918340c1be"
      },
      "source": [
        "!pip install pmdarima\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import csv\n",
        "import pmdarima as pm\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "import pandas.util.testing as tm\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from matplotlib import pyplot\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "import matplotlib._color_data as mcd\n",
        "from matplotlib.lines import Line2D\n",
        "import random\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Use seaborn style defaults and set the default figure size\n",
        "sns.set(rc={'figure.figsize':(10, 6)})\n",
        "sns.set_palette(\"tab10\", n_colors=10, desat=0.5, color_codes=True)\n",
        "color=['b','r','g','y','m']\n",
        "%matplotlib inline"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.7/dist-packages (1.8.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,>=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.29.23)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.24.3)\n",
            "Requirement already satisfied: statsmodels!=0.12.0,>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.12.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.4.1)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (57.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.0 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from pmdarima) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->pmdarima) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->pmdarima) (1.15.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.12.0,>=0.11->pmdarima) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK-74U_gH0jD"
      },
      "source": [
        "Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqCfKZY4HlVi"
      },
      "source": [
        "#Choose whether using 'testing' or 'full' dataset\n",
        "status = 'full'\n",
        "\n",
        "#Choose whether to run SARIMA analysis\n",
        "sarima_status='run'#'run'\n",
        "\n",
        "#Choose which files to read. Need to be the same price groups as eval_PG below\n",
        "sarima_load=True\n",
        "ff_file ='/content/drive/MyDrive/vindata/forward_filling_125.npy'\n",
        "s_file = '/content/drive/MyDrive/vindata/sarima_forecasts_125.npy'\n",
        "\n",
        "\n",
        "#Choose exogenous feature for SARIMAX, price or shifted sales of top ranked article\n",
        "# 'Price' or 'Top'\n",
        "#exog = 'Top'\n",
        "\n",
        "#Choose whether to load LSTM model or construct new\n",
        "LSTM_load = False\n",
        "LSTM_file = '/content/drive/MyDrive/vindata/LSTM_model_new_shape.h5'\n",
        "\n",
        "\n",
        "#Choose features\n",
        "ohe_feature=False #If set to false it is added for sample selection then removed\n",
        "price_feature=False\n",
        "utvalg_feature=False\n",
        "date_feature = False #Should not be used\n",
        "newness_feature = False\n",
        "top_feature=False\n",
        "\n",
        "#Choose price group\n",
        "train_PG=[125] #Price groups to train on\n",
        "eval_PG=[125] #Price groups to evaluate model on\n",
        "\n",
        "#Settings\n",
        "n_steps = 20 #months as input\n",
        "n_output = 3 #months to forecast\n",
        "period = 12 #seasonal period\n",
        "n_sets = 119 #len(supervised_values) \n",
        "n_test = round(n_sets*0.2)\n",
        "n_train = round((n_sets-n_test)*0.8) #the last sets are left for validation\n",
        "n_val = 13 #Nr of validation months\n",
        "\n",
        "#LSTM model\n",
        "n_epoch = 100\n",
        "n_batch = 64\n",
        "n_neurons = 64\n",
        "\n",
        "\n",
        "#number of features in addition to sales data\n",
        "n_features = 9 + n_steps*price_feature + 4*utvalg_feature + date_feature + newness_feature + n_steps*top_feature \n",
        "\n",
        "#Total width of sample before reshaping into (time,features) dimension\n",
        "n_input = n_steps + n_features - 9*(1-ohe_feature) \n",
        "\n",
        "#number of timedependent features\n",
        "t_features = 1 + price_feature + top_feature \n",
        "\n",
        "#nr features in LSTM input (batch,time,features)\n",
        "feature_width = 1 + 9*ohe_feature + 4*utvalg_feature + price_feature + date_feature + newness_feature + top_feature "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey-9uAZCqdMP"
      },
      "source": [
        "Loading data from Google Disk, concatenating data, removing rows without ranking, adding column to identify new products (within 12 months)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsq0wWUZjNEO"
      },
      "source": [
        "#Loading dataframes containing two most common red wine groups\n",
        "df75 = pd.read_csv('/content/drive/MyDrive/vindata/RedWines75.csv')\n",
        "df9 = pd.read_csv('/content/drive/MyDrive/vindata/RedWines9.csv')\n",
        "\n",
        "#Removing old index\n",
        "df75 = df75.drop('Unnamed: 0', axis=1)\n",
        "df9 = df9.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "#Setting index to yyyy-mm-dd\n",
        "df75['År_Måned'] = pd.to_datetime(df75['År_Måned'])\n",
        "#df75 = df75.set_index('År_Måned')\n",
        "df9['År_Måned'] = pd.to_datetime(df9['År_Måned'])\n",
        "#df9 = df9.set_index(['Artikkelnr','År_Måned']).sort_index()\n",
        "\n",
        "#Concatenating both dataframes, as df9 continues when df75 stops\n",
        "dfRed = pd.concat([df75, df9], axis=0)\n",
        "\n",
        "#Remove rows without ranking\n",
        "dfRed.dropna(subset = [\"Rangering\"], inplace=True)\n",
        "\n",
        "#Change values in 'Fredet' to numerical categories\n",
        "dfRed[\"Fredet\"] = dfRed[\"Fredet\"].astype('category')\n",
        "dfRed[\"Fredet\"] = dfRed[\"Fredet\"].cat.codes\n",
        "\n",
        "#Adding column with value 1 if article is new\n",
        "#New is defined as less than one year after fist sales date or jan 2007\n",
        "\n",
        "#First sales date per article\n",
        "dfFirstSale = dfRed.sort_values('År_Måned').groupby('Artikkelnr').first().reset_index()\n",
        "#Stripping df of other columns to merge with dfRed\n",
        "dfFirstSale = dfFirstSale[['Artikkelnr','År_Måned']]\n",
        "#Shifting dates by one year to compare with original År_Måned\n",
        "dfFirstSale['År_Måned'] = dfFirstSale['År_Måned'] + pd.offsets.DateOffset(years=1)\n",
        "#Changing name of column to avoid confusion when merging\n",
        "dfFirstSale.columns = ['Artikkelnr','Innført'] \n",
        "#Merging\n",
        "dfRed = dfRed.merge(dfFirstSale)\n",
        "#Constructing new column which checks if article is new for current row\n",
        "dfRed['Ny']= np.where((dfRed['Innført'] > dfRed['År_Måned']), 1, 0)\n",
        "#Removes temporary column with first sales date\n",
        "dfRed = dfRed.drop(['Innført'], axis=1)\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiaEAh1JFmNW"
      },
      "source": [
        "# Data handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGRU9b9hktGg"
      },
      "source": [
        "Splitting the data into separate price groups by making a dictionary with separate dataframes for each group. These are used for identifying products that have been within that price group, finding ranking limits, and plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30MVwVH1ZENp"
      },
      "source": [
        "#List of dates in data\n",
        "dates = dfRed['År_Måned'].tolist()\n",
        "dates = list(dict.fromkeys(dates))\n",
        "dates.sort()\n",
        "\n",
        "#List of price groups, one for indexing and one for plotting\n",
        "pg = [99,100,125,150,175,200,250,300,400]\n",
        "pg2 = train_PG #[100,125,150,175,200,250,300]\n",
        "pricegroups=['100-125','125-150','150-175','175-200','200-250','250-300','300-400']\n",
        "\n",
        "#Making a dictionary of dataframes for each price group\n",
        "dfPG={}\n",
        "dfPG[99] = dfRed[dfRed['Segmentpris']<100].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[100] = dfRed[(dfRed['Segmentpris']>=100) & (dfRed['Segmentpris']<125)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[125] = dfRed[(dfRed['Segmentpris']>=125) & (dfRed['Segmentpris']<150)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[150] = dfRed[(dfRed['Segmentpris']>=150) & (dfRed['Segmentpris']<175)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[175] = dfRed[(dfRed['Segmentpris']>=175) & (dfRed['Segmentpris']<200)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[200] = dfRed[(dfRed['Segmentpris']>=200) & (dfRed['Segmentpris']<250)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[250] = dfRed[(dfRed['Segmentpris']>=250) & (dfRed['Segmentpris']<300)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[300] = dfRed[(dfRed['Segmentpris']>=300) & (dfRed['Segmentpris']<400)].sort_values(['År_Måned','Rangering'])\n",
        "dfPG[400] = dfRed[dfRed['Segmentpris']>400].sort_values(['År_Måned','Rangering'])\n",
        "\n",
        "#Making a column for liters sold of 1st ranking wine in evaluated pricegroup  \n",
        "#for each date. This will only work as long as pricegroups are evaluated\n",
        "#seperately, which they are for this thesis  \n",
        "dfTopRankedLiters = dfPG[eval_PG[0]][dfPG[eval_PG[0]]['Rangering']==1]\n",
        "dfTopRankedLiters = dfTopRankedLiters[['År_Måned','Liter']]\n",
        "dfTopRankedLiters.columns = ['År_Måned','Liter_Top'] \n",
        "dfRed = dfRed.merge(dfTopRankedLiters)\n",
        "\n",
        "#Dataframe similar to original, but with article number and date as multiindex\n",
        "dfRed2 = dfRed.set_index(['Artikkelnr','År_Måned']).sort_index()"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsYkZWx_iJpJ"
      },
      "source": [
        "Data handling, preparations for machine learning\n",
        "- setting ranking limit to filter out least popular wines\n",
        "- listing article numbers with high enough ranking\n",
        "- preparing data frames with data for machine learning\n",
        "- making a smaller test set  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aug58JSUsNHl"
      },
      "source": [
        "#Making dataframes to shape necessary information\n",
        "\n",
        "#Returns only ranking limit and date of dataframe\n",
        "def styring(df):\n",
        "  styring = df[df['Rangering']==1]\n",
        "  return styring[['År_Måned', 'Styringstall']]\n",
        "\n",
        "#Data frame for machine learning\n",
        "\n",
        "#List of ranking limit we will use when choosing article numbers\n",
        "limits = []\n",
        "if status=='full': \n",
        "  for price in pg2:\n",
        "    limits.append(round(max(styring(dfPG[price])['Styringstall'])*1.2))\n",
        "elif status=='testing':\n",
        "  for price in pg2:\n",
        "    limits.append(10)\n",
        "else:\n",
        "  print(\"Choose between 'testing' or 'full' dataset\") \n",
        "\n",
        "#List of articles that have had a ranking value lower than the limit found above\n",
        "articles = []\n",
        "for i in range(len(train_PG)):\n",
        "  top = dfPG[train_PG[i]][dfPG[train_PG[i]]['Rangering']<=limits[i]]\n",
        "  top = list(dict.fromkeys(top['Artikkelnr'].tolist()))\n",
        "  articles += top\n",
        "articles = list(dict.fromkeys(articles))\n",
        "\n",
        "#Constructing dataframes with articles as columns, date as row index and netto\n",
        "#sales, price and quarantene-status, etc. as values. The first article decides \n",
        "#length/dates of the dataframes, therefore this must have data for all months\n",
        "dfA = pd.DataFrame()\n",
        "dfP = pd.DataFrame()\n",
        "dfQ = pd.DataFrame()\n",
        "dfU = pd.DataFrame()\n",
        "dfN = pd.DataFrame()\n",
        "dfT = pd.DataFrame()\n",
        "dfRank = pd.DataFrame()\n",
        "\n",
        "first_article=False\n",
        "for art in articles:\n",
        "  #Skips articles until one of proper length appears.\n",
        "  if len(dfRed2.loc[art]['Netto Salg'].index)==153:\n",
        "    first_article=True\n",
        "  if first_article==True:\n",
        "    temp_art = dfRed2.loc[art]['Netto Salg']\n",
        "    #print(len(temp_art.index))\n",
        "    temp_art.reindex(dates, fill_value=0)\n",
        "    #print(len(temp_art.index))\n",
        "    dfA[art] = temp_art\n",
        "    temp_price = dfRed2.loc[art]['Segmentpris']\n",
        "    temp_price.reindex(dates, fill_value=0)\n",
        "    dfP[art] = temp_price\n",
        "    temp_fredet = dfRed2.loc[art]['Fredet']\n",
        "    temp_fredet.reindex(dates, fill_value=0)\n",
        "    dfQ[art] = temp_fredet\n",
        "    temp_utvalg = dfRed2.loc[art]['Utvalg']\n",
        "    temp_utvalg.reindex(dates, fill_value=0)\n",
        "    dfU[art] = temp_utvalg\n",
        "    temp_ny = dfRed2.loc[art]['Ny']\n",
        "    temp_ny.reindex(dates, fill_value=0)\n",
        "    dfN[art] = temp_ny\n",
        "    temp_top = dfRed2.loc[art]['Liter_Top']\n",
        "    temp_top.reindex(dates, fill_value=0)\n",
        "    dfT[art] = temp_top\n",
        "    temp_rank = dfRed2.loc[art]['Rangering']\n",
        "    temp_rank.reindex(dates, fill_value=0)\n",
        "    dfRank[art] = temp_rank\n",
        "\n",
        "\n",
        "#Replacing NaN with 0.\n",
        "dfA = dfA.fillna(0)\n",
        "dfP = dfP.fillna(0)\n",
        "dfQ = dfQ.fillna(0)\n",
        "dfN = dfN.fillna(0)\n",
        "dfT = dfT.fillna(0)\n",
        "dfRank = dfRank.fillna(0)\n",
        "\n",
        "#Constructing a test data frame from n wines\n",
        "\n",
        "#All wines are used right now, but can be used to limit products\n",
        "wines = list(dfA.columns)\n",
        "np.random.shuffle(wines) \n",
        "\n",
        "#Makes a smaller test set of specific wines, uses all right now\n",
        "dfTest = dfA[wines]\n",
        "dfTestP = dfP[wines]\n",
        "dfTestQ = dfQ[wines]\n",
        "dfTestU = dfU[wines]\n",
        "dfTestN = dfN[wines]\n",
        "dfTestT = dfT[wines]\n",
        "dfTestRank = dfRank[wines]\n",
        "\n",
        "#Resetting index  \n",
        "dfTestP = dfTestP.reset_index()\n",
        "dfTestQ = dfTestQ.reset_index()\n",
        "dfTestU = dfTestU.reset_index()\n",
        "dfTestN = dfTestN.reset_index()\n",
        "dfTestT = dfTestT.reset_index()\n",
        "dfTestRank = dfTestRank.reset_index()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSd-ULAUCVtA"
      },
      "source": [
        "Splitting up into training, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1K3dsUA-6Md",
        "outputId": "fd611d4e-2b0f-42ae-b6e3-575376ea5b77"
      },
      "source": [
        "# Scaling data in data frame over all values, not columnwise\n",
        "def scale(df):\n",
        "  x = df.values \n",
        "  x_flat = df.values.flatten().reshape(-1, 1)\n",
        "  min_max_scaler = MinMaxScaler()\n",
        "  min_max_scaler.fit(x_flat)\n",
        "  x_scaled = min_max_scaler.transform(x)\n",
        "  return min_max_scaler, pd.DataFrame(x_scaled, columns= df.columns)\n",
        "\n",
        "#Taking seasonl difference, scaling data, and removing first rows with nan values\n",
        "#from the differencing\n",
        "def prepare_dataframe(df, period):\n",
        "  #Differencing each column in dataframe (not in use, removing 12 months to compensate)\n",
        "  #df = df.diff(periods=period)\n",
        "  df = df.iloc[12:]\n",
        "  #Scaling data frame\n",
        "  scaler, df = scale(df)\n",
        "  #Removing rows with nan values\n",
        "  df = df.dropna(axis=0, how='all')\n",
        "  return df, scaler\n",
        "\n",
        "# convert time series into supervised learning problem \n",
        "# an adaption of function from Jason Brownlee\n",
        "def series_to_supervised(df, n_in=1, n_out=1, dropnan=True):\n",
        "  columns= df.columns\n",
        "  dfS = {}\n",
        "  for col in df.columns:\n",
        "    data = df[col]\n",
        "    n_vars = 1 \n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "      cols.append(data.shift(i))\n",
        "      names += [('%d(t-%d)' % (col, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(data.shift(-i))\n",
        "      if i == 0:\n",
        "        names += [('%d(t)' % (col)) for j in range(n_vars)]\n",
        "      else:\n",
        "        names += [('%d(t+%d)' % (col, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.DataFrame()\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "      agg.dropna(inplace=True)\n",
        "    dfS[col] = agg\n",
        "  return dfS\n",
        "\n",
        "#One hot encoding price groups\n",
        "def one_hot_price(price):\n",
        "  if price < 100:\n",
        "    return [1,0,0,0,0,0,0,0,0]\n",
        "  if (100 <= price <125):\n",
        "    return [0,1,0,0,0,0,0,0,0]\n",
        "  if (125 <= price <150):\n",
        "    return [0,0,1,0,0,0,0,0,0]\n",
        "  if (150 <= price <175):\n",
        "    return [0,0,0,1,0,0,0,0,0]\n",
        "  if (175 <= price <200):\n",
        "    return [0,0,0,0,1,0,0,0,0]\n",
        "  if (200 <= price <250):\n",
        "    return [0,0,0,0,0,1,0,0,0]\n",
        "  if (250 <= price <300):\n",
        "    return [0,0,0,0,0,0,1,0,0]\n",
        "  if (300 <= price <400):\n",
        "    return [0,0,0,0,0,0,0,1,0]\n",
        "  else:\n",
        "    return [0,0,0,0,0,0,0,0,1]\n",
        "\n",
        "#One hot encoding product selection\n",
        "def one_hot_utvalg(utvalg):\n",
        "  if (utvalg=='Basisutvalget'):\n",
        "    return [1,0,0,0]\n",
        "  if (utvalg=='Bestillingsutvalget'):\n",
        "    return [0,1,0,0]\n",
        "  if (utvalg=='Testutvalget'):\n",
        "    return [0,0,1,0]\n",
        "  else:\n",
        "    return [0,0,0,1]  \n",
        "\n",
        "\n",
        "#Constructing a mapping of dates from integer to month-year, 0 is 01-2007\n",
        "date_map = {}\n",
        "for i in range(len(dates)):\n",
        "  date_map[i]=dates[i]\n",
        "#Reverse  map\n",
        "inv_date_map = {v: k for k, v in date_map.items()}\n",
        "\n",
        "\n",
        "#Differencing and scaling data\n",
        "dfPrep, scaler = prepare_dataframe(dfTest,12) \n",
        "dfPrepPrice, scalerPrice = prepare_dataframe(dfP[wines],0) #Will work s long as differencing is removed, has to be adapted later if differencing is added again\n",
        "dfPrepTop, scalerTop = prepare_dataframe(dfT[wines],0)\n",
        "\n",
        "#Splitting the time series into time steps \n",
        "dfSup = series_to_supervised(dfPrep, n_steps, n_output, True)\n",
        "dfSupPrice = series_to_supervised(dfPrepPrice, n_steps, n_output, True)\n",
        "dfSupTop = series_to_supervised(dfPrepTop, n_steps, n_output, True)\n",
        "dfActual = series_to_supervised(dfTest, n_steps, n_output, True)\n",
        "\n",
        "#Dataframe without first year, as this was reserved for differencing. \n",
        "dfA_short = dfTest.iloc[12:] \n",
        "\n",
        "#Splitting into training, validation and test sets, leaving a 6 month quarantene\n",
        "#between validation and test.\n",
        "\n",
        "#Dates are referred to by months since january 2007, the first date of raw data.\n",
        "f_date = period+n_steps-1\n",
        "l_date = 152-n_output\n",
        "dates_short = np.linspace(f_date,l_date, l_date-f_date+1)\n",
        "\n",
        "#Empty arrays\n",
        "train = np.zeros((76*len(dfSup), (n_steps + n_features+n_output+2)))\n",
        "val = np.zeros((13*len(dfSup), (n_steps + n_features+n_output+2)))\n",
        "test = np.zeros((24*len(dfSup), (n_steps + n_features+n_output+2)))\n",
        "test_actual = np.zeros((24*len(dfSup), (n_steps+n_output))) #+4? input i stedet?\n",
        "\n",
        "#Separate count for train, val, and test\n",
        "j,k,l = 0,0,0\n",
        "\n",
        "#Function that puts together features into a single array\n",
        "def list_features(start,stop,article,test):\n",
        "\n",
        "  if test==True:\n",
        "    length =  len(range(int(dates_short[start]),int(dates_short[stop]+1)))\n",
        "  else:\n",
        "    length =  len(range(int(dates_short[start]),int(dates_short[stop])))\n",
        "\n",
        "  all_features=np.zeros((length,n_features))\n",
        "  f_count=0\n",
        "\n",
        "  #Price group features setup\n",
        "  if test==True:\n",
        "    all_features[:,f_count:(f_count+9)] = [one_hot_price(dfTestP[article].iloc[m]) for m in range(int(dates_short[start]),int(dates_short[stop]+1))]\n",
        "  else:\n",
        "    all_features[:,f_count:(f_count+9)] = [one_hot_price(dfTestP[article].iloc[m]) for m in range(int(dates_short[start]),int(dates_short[stop]))]\n",
        "  f_count+=9\n",
        "  \n",
        "  #Selection feature setup \n",
        "  if utvalg_feature==True:\n",
        "    if test==True:\n",
        "      all_features[:,f_count:(f_count+4)] = [one_hot_utvalg(dfTestU[article].iloc[m]) for m in range(int(dates_short[start]),int(dates_short[stop]+1))]\n",
        "    else:\n",
        "      all_features[:,f_count:(f_count+4)] = [one_hot_utvalg(dfTestU[article].iloc[m]) for m in range(int(dates_short[start]),int(dates_short[stop]))]\n",
        "    f_count+=4\n",
        "\n",
        "  #Date feature setup\n",
        "  if date_feature==True:\n",
        "\n",
        "    #Normalizing dates to use as feature in range[0,1]\n",
        "    dates_normalized = np.array([(x-31)/(149-31) for x in dates_short])\n",
        "    dates_normalized = dates_normalized.reshape(len(dates_normalized),1)\n",
        "\n",
        "    if test==True:\n",
        "      all_features[:,f_count:(f_count+1)] = dates_normalized[start:]\n",
        "    else:\n",
        "      all_features[:,f_count:(f_count+1)] = dates_normalized[start:stop]\n",
        "    f_count+=1\n",
        "  \n",
        "  #Newness feture setup\n",
        "  if newness_feature==True:\n",
        "\n",
        "    if test==True:\n",
        "      newness_features = [dfTestN[article].iloc[m] for m in range(int(dates_short[start]),int(dates_short[stop]+1))]\n",
        "      all_features[:,f_count:(f_count+1)] = np.array(newness_features).reshape(len(newness_features),1) \n",
        "    else:\n",
        "      newness_features = [dfTestN[article].iloc[m] for m in range(int(dates_short[start]),int(dates_short[stop]))]\n",
        "      all_features[:,f_count:(f_count+1)] = np.array(newness_features).reshape(len(newness_features),1)\n",
        "    f_count+=1\n",
        "\n",
        "  #Price feature setup\n",
        "  if price_feature==True:\n",
        "    price_features=dfSupPrice[art]\n",
        "    price_features = price_features.iloc[:, :-3] #drop last 3 columns, they were values to forecast\n",
        "    \n",
        "    if test==True:\n",
        "      all_features[:,f_count:(f_count+n_steps)] = price_features.values[start:]\n",
        "    else:\n",
        "      all_features[:,f_count:(f_count+n_steps)] = price_features.values[start:stop]\n",
        "    f_count+=n_steps\n",
        "\n",
        "  #Liters of top ranked wine feature setup\n",
        "  if top_feature==True:\n",
        "    top_features=dfSupTop[art]\n",
        "    top_features = top_features.iloc[:, :-3] #drop last 3 columns, they were values to forecast\n",
        "    \n",
        "    if test==True:\n",
        "      all_features[:,f_count:(f_count+n_steps)] = top_features.values[start:]\n",
        "    else:\n",
        "      all_features[:,f_count:(f_count+n_steps)] = top_features.values[start:stop]\n",
        "    f_count+=n_steps\n",
        "\n",
        "  return all_features\n",
        "\n",
        "\n",
        "#Looping through each wine and adding features, article nr, and date to samples\n",
        "keys = list(dfSup.keys())\n",
        "for art in keys:\n",
        "  #Extracting values from data frame\n",
        "  supervised_values = dfSup[art].values\n",
        "  supervised_price = dfSupPrice[art].values\n",
        "  actual_values = dfActual[art].values\n",
        "\n",
        "  #Train\n",
        "  train[j:(j+n_train),0:n_features] = list_features(0,n_train,art,False)\n",
        "  train[j:(j+n_train),n_features:(n_features+n_steps+n_output)] = supervised_values[0:n_train]\n",
        "  train[j:(j+n_train),(n_features+n_steps+n_output)] = art\n",
        "  train[j:(j+n_train),(n_features+n_steps+n_output+1)] = dates_short[0:n_train]\n",
        "  j += n_train\n",
        "  #Validation\n",
        "  val[k:(k+n_val),0:n_features] = list_features(n_train,-(n_test+6),art,False)\n",
        "  val[k:(k+n_val),n_features:(n_features+n_steps+n_output)] = supervised_values[n_train:-(n_test+6)]\n",
        "  val[k:(k+n_val),(n_features+n_steps+n_output)] = art\n",
        "  val[k:(k+n_val),(n_features+n_steps+n_output+1)] = dates_short[n_train:-(n_test+6)]\n",
        "  k += n_val\n",
        "  #Test\n",
        "  test[l:(l+n_test),0:n_features] = list_features(-n_test,-1,art,True)\n",
        "  test[l:(l+n_test),n_features:(n_features+n_steps+n_output)] = supervised_values[-n_test:]\n",
        "  test[l:(l+n_test),(n_features+n_steps+n_output)] = art\n",
        "  test[l:(l+n_test),(n_features+n_steps+n_output+1)] = dates_short[-n_test:]\n",
        "  #Test values without preprocessing\n",
        "  test_actual[l:(l+n_test),0:23] = actual_values[-n_test:]\n",
        "  l += n_test\n",
        "\n",
        "#Shuffling order of samples\n",
        "np.random.shuffle(train)\n",
        "np.random.shuffle(val)\n",
        "\n",
        "#Removes test samples from price group only used in training \n",
        "#uses one hot encoded feature\n",
        "def remove_test_samples(test,test_actual):\n",
        "  #Gets the onehot encoding for the price groups that will be evaluated\n",
        "  keep_ind = [one_hot_price(price).index(1) for price in eval_PG]\n",
        "\n",
        "  #Constructs a mask that is false for all samples in test that are not in correct price group\n",
        "  mask = np.ones(len(test), dtype=bool)\n",
        "  for pos,val in enumerate(test):\n",
        "    if list(val).index(1) not in keep_ind:\n",
        "      mask[pos] = False\n",
        "\n",
        "  #Uses mask on test samples and actual test values we wish to forecast upon return\n",
        "  return test[mask], test_actual[mask]\n",
        "\n",
        "test, test_actual = remove_test_samples(test, test_actual)\n",
        "\n",
        "#Removes onehot encoded price group feature if set to false\n",
        "if ohe_feature==False:\n",
        "  train = train[:,9:]\n",
        "  val = val[:,9:]\n",
        "  test = test[:,9:]\n",
        "\n",
        "#Resampling to make nr samples in train and val divisible by n_batch\n",
        "add_train = n_batch- len(train)%n_batch\n",
        "train = np.vstack((train, train[:add_train]))\n",
        "\n",
        "add_val = n_batch- len(val)%n_batch\n",
        "val = np.vstack((val, val[:add_val]))\n",
        "\n",
        "#Number of samples, originally 76 train, 13 val, 24 test for each wine\n",
        "print('Training samples: ',len(train))\n",
        "print('Validation samples: ',len(val))\n",
        "print('Test samples: ',len(test))\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples:  47680\n",
            "Validation samples:  8192\n",
            "Test samples:  3382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6O4CsKFFswx"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm_cwAqVHaT3"
      },
      "source": [
        "Fitting and forecasting LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjU4zsfCHf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77e4834-587b-47d4-f1b2-14e4ebf4612f"
      },
      "source": [
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, val, n_input, n_steps, n_seq, n_batch, nb_epoch, n_neurons):\n",
        "  # reshape training into [samples, timesteps, features]\n",
        "  X, y = train[:, 0:n_input], train[:, n_input:(n_input+n_seq)]\n",
        "  X = shape_X(X, X.shape[0],n_steps, feature_width)\n",
        "\n",
        "  #Reshape validation samples\n",
        "  Xval, yval = val[:, 0:n_input], val[:, n_input:(n_input+n_seq)]\n",
        "  Xval = shape_X(Xval, Xval.shape[0],n_steps, feature_width)\n",
        "\n",
        "  print('Batch size: ',n_batch)\n",
        "  print('Time: ',X.shape[1])\n",
        "  print('Features: ',X.shape[2])\n",
        "  print('Output size:', y.shape[1])\n",
        "\n",
        "  # design network\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_neurons,return_sequences=True, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=False))\n",
        "  model.add(LSTM(n_neurons, stateful=False))\n",
        "  model.add(Dense(y.shape[1],activation= None))\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) #Default lr is 0.001\n",
        "  model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['accuracy'])\n",
        "  history = model.fit(X, y, validation_data =(Xval, yval), epochs=n_epoch, batch_size=n_batch, callbacks=[callback], verbose=1, shuffle=False)\n",
        "  return model, history\n",
        "\n",
        "#function to reshape X of shape [[feature_1,feature_2,...,sales_1,...sales_20]]\n",
        "#into [[feature_1,feature_2,...,sales_1]...[feature_1,feature_2,...,sales_20]]\n",
        "#need to update to take price as input as well, giving\n",
        "#[[feature_1,feature_2,...,price_1,sales_1]...[feature_1,feature_2,...,price_20,sales_20]]\n",
        "def shape_X(X,n_batch,time,features):\n",
        "  #For forescasting, when a single sample is processed at a time\n",
        "  if X.ndim==1:\n",
        "    return shape_single_X(X,time,features)\n",
        "  #For multiple samples\n",
        "  all_samples=[]\n",
        "  for s in range(len(X)):\n",
        "    sample=np.zeros((time,features))\n",
        "    #Fills up constant features and leaves time dependent columns empty at end\n",
        "    sample[:,:(features-t_features)]=X[s,:(features-t_features)]\n",
        "    for i in range(time):\n",
        "      for tf in range(t_features):\n",
        "        sample[i,features-t_features+tf]=X[s,(features-t_features+i+tf*time)]\n",
        "    all_samples.append(sample)\n",
        "  return np.array(all_samples)\n",
        "\n",
        "#does same as shape_X but for a single input sample, called from shape_X\n",
        "def shape_single_X(X,time,features):\n",
        "  sample=np.zeros((time,features))\n",
        "  sample[:,:(features-t_features)]=X[:(features-t_features)]\n",
        "  for i in range(time):\n",
        "    for tf in range(t_features):\n",
        "      sample[i,(features-t_features+tf)]=X[(features-t_features+i+tf*time)]\n",
        "  return sample.reshape(1,time,features)\n",
        "\n",
        "# make one forecast with an LSTM,\n",
        "def forecast_lstm(model, X, n_batch, n_steps, n_input):\n",
        "  # reshape input pattern to [samples, timesteps, features]\n",
        "  X = shape_X(X, 1,n_steps, feature_width) \n",
        "  # make forecast\n",
        "  forecast = model.predict(X, batch_size=n_batch)\n",
        "  # convert to array\n",
        "  return [x for x in forecast[0, :]]\n",
        "\n",
        "# evaluate the persistence model\n",
        "def make_forecasts(model, n_batch, train, test, n_input, n_seq, n_steps): #må fjerne argumenter her\n",
        "  forecasts = list()\n",
        "  #Update model for online forecasting\n",
        "  # re-define the batch size\n",
        "  n_batch = 1\n",
        "  # re-define model\n",
        "  new_model = Sequential()\n",
        "  new_model.add(LSTM(n_neurons,return_sequences=True, batch_input_shape=(n_batch, n_steps, feature_width), stateful=False)) #\n",
        "  new_model.add(LSTM(n_neurons, stateful=False))\n",
        "  new_model.add(Dense(n_output,activation= None))\n",
        "  # copy weights\n",
        "  old_weights = model.get_weights()\n",
        "  new_model.set_weights(old_weights)\n",
        "  for i in range(len(test)):\n",
        "    X, y = test[i, 0:n_input], test[i, n_input:]\n",
        "    # make forecast\n",
        "    forecast = forecast_lstm(new_model, X, n_batch,n_steps, n_input)\n",
        "    # store the forecast\n",
        "    forecasts.append(forecast)\n",
        "  return forecasts\n",
        "\n",
        "#Function that loads model or fits a new model and saves it\n",
        "def LSTM_model(load=True):\n",
        "  if load==True:\n",
        "    model = load_model(LSTM_file)\n",
        "    history = 0\n",
        "  else:\n",
        "    # fit model\n",
        "    model, history = fit_lstm(train, val, n_input, n_steps, n_output, n_batch, n_epoch, n_neurons)\n",
        "\n",
        "    #Saving model for future use\n",
        "    model.save('LSTM_model.h5')\n",
        "  return model, history \n",
        "\n",
        "model, history = LSTM_model(load=LSTM_load)\n",
        "\n",
        "# constructing forecasts from model\n",
        "forecasts = make_forecasts(model, n_batch, train, test, n_input, n_output, n_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch size:  64\n",
            "Time:  20\n",
            "Features:  1\n",
            "Output size: 3\n",
            "Epoch 1/100\n",
            "745/745 [==============================] - 24s 28ms/step - loss: 0.0101 - accuracy: 0.3477 - val_loss: 0.0057 - val_accuracy: 0.2700\n",
            "Epoch 2/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0042 - accuracy: 0.4363 - val_loss: 0.0046 - val_accuracy: 0.7480\n",
            "Epoch 3/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0038 - accuracy: 0.4655 - val_loss: 0.0045 - val_accuracy: 0.7695\n",
            "Epoch 4/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0037 - accuracy: 0.4594 - val_loss: 0.0045 - val_accuracy: 0.3392\n",
            "Epoch 5/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0035 - accuracy: 0.4610 - val_loss: 0.0042 - val_accuracy: 0.3312\n",
            "Epoch 6/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0035 - accuracy: 0.4670 - val_loss: 0.0042 - val_accuracy: 0.2867\n",
            "Epoch 7/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0034 - accuracy: 0.4918 - val_loss: 0.0043 - val_accuracy: 0.2822\n",
            "Epoch 8/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0033 - accuracy: 0.4741 - val_loss: 0.0041 - val_accuracy: 0.7921\n",
            "Epoch 9/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0032 - accuracy: 0.4759 - val_loss: 0.0043 - val_accuracy: 0.3195\n",
            "Epoch 10/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0031 - accuracy: 0.4776 - val_loss: 0.0040 - val_accuracy: 0.7712\n",
            "Epoch 11/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0031 - accuracy: 0.4437 - val_loss: 0.0038 - val_accuracy: 0.2728\n",
            "Epoch 12/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0030 - accuracy: 0.4766 - val_loss: 0.0037 - val_accuracy: 0.7733\n",
            "Epoch 13/100\n",
            "745/745 [==============================] - 20s 27ms/step - loss: 0.0030 - accuracy: 0.4666 - val_loss: 0.0038 - val_accuracy: 0.7717\n",
            "Epoch 14/100\n",
            "465/745 [=================>............] - ETA: 7s - loss: 0.0029 - accuracy: 0.4777"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiG43o4UvXFF"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7c2LWADPUXL"
      },
      "source": [
        "Evaluation of forecast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2NPYwhbu099"
      },
      "source": [
        "# inverse differenced forecast\n",
        "def inverse_difference(actual, forecast, period):\n",
        "  for i in range(len(forecast)):\n",
        "    forecast[i]= forecast[i] + actual[-(len(forecast)+period-i)]\n",
        "  return forecast\n",
        "\n",
        "# inverse data transform on forecasts, adapted for seasonal differencing\n",
        "# and multiple series\n",
        "def inverse_transform_all(actual, forecasts, scaler, period):\n",
        "  inverted = list()\n",
        "  for i in range(len(forecasts)):\n",
        "    # create array from forecast\n",
        "    forecast = array(forecasts[i])\n",
        "    act = array(actual[i])\n",
        "    forecast = forecast.reshape(1, len(forecast))\n",
        "    # invert scaling\n",
        "    inv_scale = scaler.inverse_transform(forecast)\n",
        "    inv_scale = inv_scale[0, :]\n",
        "    # invert differencing \n",
        "\n",
        "    #index = len(series) - n_test + i - period\n",
        "    inv_diff = inv_scale \n",
        "    # store\n",
        "    inverted.append(inv_diff)\n",
        "  return inverted\n",
        "\n",
        "# evaluate the forecast with MAE\n",
        "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
        "  for i in range(n_seq):\n",
        "    actual = [row[i] for row in test]\n",
        "    predicted = [forecast[i] for forecast in forecasts]\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    print('t+%d MAE: %f' % ((i+1), mae))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTvEh_egWXVV"
      },
      "source": [
        "#LSTM evaluation\n",
        "\n",
        "# inverse transform forecasts\n",
        "LSTM_forecasts = inverse_transform_all(test_actual, forecasts, scaler, period)\n",
        "test_actual_output = [list(row[n_steps:]) for row in test_actual]\n",
        "\n",
        "# evaluate forecasts\n",
        "evaluate_forecasts(test_actual_output, LSTM_forecasts, n_steps, n_output) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqROLsNWF0dB"
      },
      "source": [
        "# Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F432lbTrIWr7"
      },
      "source": [
        "Making SARIMA forecasts and using forward filling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSFu6ODJyNb0"
      },
      "source": [
        "#Preparing data for SARIMA model\n",
        "\n",
        "#Dates to use for input\n",
        "dates_short_2=dfA_short.index\n",
        "\n",
        "#Dates to use for exogenous variable shifted 3 months\n",
        "dates_short_2_shifted = dates_short_2.shift(-3, freq='MS')\n",
        "\n",
        "#Getting the article numbers used for LSTM models\n",
        "cols = dfA_short.columns\n",
        "\n",
        "#Price as exogenous values\n",
        "dfP_short = dfTestP.iloc[12:]\n",
        "dfP_short.index = dfP_short['År_Måned']\n",
        "\n",
        "#Price as exogenous values\n",
        "dfT_short = dfTestT.iloc[9:-3]\n",
        "dfT_short.index = dfT_short['År_Måned']\n",
        "\n",
        "S_X = {}\n",
        "S_Y = {}\n",
        "S_XX = {}\n",
        "S_XY = {}\n",
        "S_names = []\n",
        "\n",
        "for col in cols:\n",
        "  for i in range(n_test):\n",
        "    name = col*1000+i+1\n",
        "    S_names.append(name)\n",
        "    S_X[name] = dfA_short[col][:(len(dates_short_2)-n_test-n_output+i+1)]\n",
        "    S_Y[name] = dfA_short[col][(len(dates_short_2)-n_test-n_output+i+1):(len(dates_short_2)-n_test-n_output+i+3+1)]\n",
        "    #Price as exogenous value\n",
        "    if exog=='Price':\n",
        "      S_XX[name] = dfP_short[col][:(len(dates_short_2)-n_test-n_output+i+1)]\n",
        "      S_XY[name] = dfP_short[col][(len(dates_short_2)-n_test-n_output+i+1):(len(dates_short_2)-n_test-n_output+i+3+1)]\n",
        "\n",
        "    #Sales of top ranked article as exogenous value\n",
        "    if exog=='Top':\n",
        "      S_XX[name] = dfT_short[col][:(len(dates_short_2_shifted)-n_test-n_output+i+1)]\n",
        "      S_XY[name] = dfT_short[col][(len(dates_short_2_shifted)-n_test-n_output+i+1):(len(dates_short_2_shifted)-n_test-n_output+i+3+1)]\n",
        "\n",
        "      #Shifting dates back to match SARIMA model    \n",
        "      S_XX[name] = S_XX[name].shift(3, freq='MS')\n",
        "      S_XY[name] = S_XY[name].shift(3, freq='MS')\n",
        "\n",
        "#Ignoring convergence warnings to avoid printing too many lines\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "\n",
        "\n",
        "def baselines(load=True):\n",
        "  if load==True:\n",
        "    forward_filling = np.load(ff_file)\n",
        "    s_forecasts = np.load(s_file)\n",
        "  else:\n",
        "    s_forecasts = []\n",
        "    forward_filling = []\n",
        "    arima = pm.ARIMA(order=(1, 1, 0), seasonal_order=(1, 0, 1, 12))\n",
        "\n",
        "    for j in range(len(cols)):\n",
        "      print(j)\n",
        "\n",
        "      try:\n",
        "        arima.fit(S_X[S_names[0+j*n_test]], X = array(S_XX[S_names[j*n_test]]).reshape(-1, 1)) #\n",
        "        pred = arima.predict(n_periods=3, X=array(S_XY[S_names[j*n_test]]).reshape(-1,1)) #\n",
        "        s_forecasts.append(pred)\n",
        "        forward_filling.append([S_X[S_names[0+j*n_test]][-1],\n",
        "                                S_X[S_names[0+j*n_test]][-1],\n",
        "                                S_X[S_names[0+j*n_test]][-1]])\n",
        "\n",
        "        for i in range(23):\n",
        "          try: \n",
        "            arima.update(S_X[S_names[i+1+j*n_test]][-1], X=array(S_XX[S_names[i+1+j*n_test]][-1]).reshape(-1,1)) #\n",
        "            pred = arima.predict(n_periods=3,X=array(S_XY[S_names[i+1+j*n_test]]).reshape(-1,1)) #\n",
        "            s_forecasts.append(pred)\n",
        "          except:\n",
        "            print(j,i)   \n",
        "            s_forecasts.append([S_X[S_names[0+j*n_test]][-1],\n",
        "                                S_X[S_names[0+j*n_test]][-1],\n",
        "                                S_X[S_names[0+j*n_test]][-1]])\n",
        "            \n",
        "          forward_filling.append([S_X[S_names[i+1+j*n_test]][-1],\n",
        "                                S_X[S_names[i+1+j*n_test]][-1],\n",
        "                                S_X[S_names[i+1+j*n_test]][-1]])\n",
        "          \n",
        "      except:\n",
        "        for i in range(24):\n",
        "          print('forward filling instead')\n",
        "          s_forecasts.append([S_X[S_names[i+j*n_test]][-1],\n",
        "                                S_X[S_names[i+j*n_test]][-1],\n",
        "                                S_X[S_names[i+j*n_test]][-1]])\n",
        "          forward_filling.append([S_X[S_names[i+j*n_test]][-1],\n",
        "                                S_X[S_names[i+j*n_test]][-1],\n",
        "                                S_X[S_names[i+j*n_test]][-1]])\n",
        "\n",
        "\n",
        "    np.save('sarimax_forecasts.npy',s_forecasts)\n",
        "    np.save('forward_filling.npy',forward_filling)\n",
        "  return s_forecasts, forward_filling\n",
        "\n",
        "\n",
        "if sarima_status=='run': \n",
        "\n",
        "  s_forecasts, forward_filling = baselines(load=sarima_load) \n",
        "\n",
        "  #The forward filling and SARIMA models are run on samples that are not to be \n",
        "  #evaluted, as these are in the wrong price group. These have to be removed after\n",
        "  #forecasting rather than before as for LSTM, as the used update method depends \n",
        "  #on continuous samples.\n",
        "\n",
        "  test_names=[]\n",
        "  for i in range(len(test)):\n",
        "    name = int(test[i][-2]*1000+(test[i][-1]-125))\n",
        "    test_names.append(name)\n",
        "\n",
        "  #Constructing mask for samples to evaluate\n",
        "  name_mask = np.in1d(S_names, test_names)\n",
        "\n",
        "  #Changes array of array into 2d array\n",
        "  s_forecasts = np.stack(s_forecasts)\n",
        "  forward_filling = np.stack(forward_filling)\n",
        "\n",
        "  #Uses mask on forecasts, only correct samples are left\n",
        "  s_forecasts = s_forecasts[name_mask] \n",
        "  forward_filling = forward_filling[name_mask] \n",
        "\n",
        "  #SARIMA evaluation\n",
        "  test_actual_output = [list(row[n_steps:]) for row in test_actual]\n",
        "  print(evaluate_forecasts(test_actual_output, s_forecasts, n_steps, n_output))\n",
        "\n",
        "  #Forward filling evaluation\n",
        "  print(evaluate_forecasts(test_actual_output, forward_filling, n_steps, n_output))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLF-e3qRF4gE"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xupxV24qwUR"
      },
      "source": [
        "Choose which model you want to analyze below. Press ctrl+F10 after changing variable to run the cells from here on down. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGC6yO6kvO6W"
      },
      "source": [
        "#LSTM_forecasts for LSTM, s_forecasts for SARIMA or forward_filling for \n",
        "#persistence forecast\n",
        "chosen_forecast=LSTM_forecasts \n",
        "\n",
        "#Adding forecasts to test samples, to keep track of dates and article numbers \n",
        "forecast_list = [list(row) for row in chosen_forecast]\n",
        "test_list = [list(row) for row in test]\n",
        "forecast_samples = [a + b for a, b in zip(test_list, forecast_list)]\n",
        "\n",
        "#Dataframe of samples with forecast\n",
        "dfForecast = pd.DataFrame(forecast_samples)\n",
        "dfForecast.drop(dfForecast.iloc[:, 0:(n_input+n_output)], inplace = True, axis = 1)\n",
        "dfForecast.columns = ['Artikkelnr', 'År_Måned', 'F1', 'F2', 'F3']\n",
        "dfForecast['Artikkelnr'] = dfForecast['Artikkelnr'].astype(int)\n",
        "dfForecast['År_Måned'] = dfForecast['År_Måned'].apply(lambda x: date_map[x])\n",
        "test_dates = list(dict.fromkeys(dfForecast['År_Måned']))\n",
        "\n",
        "#Making a dataframe with information about current date, and the next three months\n",
        "#Using date and article number to merge forecasts with relevant columns, then\n",
        "# shifting the columns by one month and adding them again\n",
        "def shift_date(df,months):\n",
        "  df = df.reset_index()\n",
        "  df['År_Måned'] = df['År_Måned'].apply(lambda x: inv_date_map[x])\n",
        "  df['År_Måned']=df['År_Måned']-months\n",
        "  df = df[(df['År_Måned']>0)&(df['År_Måned']<153)]\n",
        "  df['År_Måned'] = df['År_Måned'].apply(lambda x: date_map[x])\n",
        "  df = df.set_index(['Artikkelnr', 'År_Måned']).sort_index()\n",
        "  return df\n",
        "\n",
        "dfForecast = dfForecast.set_index(['Artikkelnr', 'År_Måned']).sort_index()\n",
        "dfInfo = dfRed2[['Segmentpris', 'Netto Salg', 'Rangering']]\n",
        "dfForecast = dfForecast.merge(dfInfo, 'left', left_index=True, right_index=True)\n",
        "dfInfo = shift_date(dfInfo,1)\n",
        "dfForecast = dfForecast.merge(dfInfo, 'left', left_index=True, right_index=True)\n",
        "dfInfo = shift_date(dfInfo,1)\n",
        "dfForecast = dfForecast.merge(dfInfo, 'left', left_index=True, right_index=True)\n",
        "dfInfo = shift_date(dfInfo,1)\n",
        "dfForecast = dfForecast.merge(dfInfo, 'left', left_index=True, right_index=True)\n",
        "rlInfo = dfRed2[['Styringstall']]\n",
        "dfForecast = dfForecast.merge(rlInfo, 'left', left_index=True, right_index=True)\n",
        "dfForecast.columns = [ 'F1', 'F2', 'F3', 'P0', 'S0', 'R0', 'P1', 'S1', 'R1', 'P2', 'S2', 'R2', 'P3', 'S3', 'R3', 'RL']\n",
        "dfForecast = dfForecast.reindex(sorted(dfForecast.columns), axis=1)\n",
        "dfForecast = dfForecast.reset_index()\n",
        "\n",
        "\n",
        "#Column that shows 1 if in basisutvalget(or below set limit) and 0 otherwise\n",
        "dfForecast['B0'] = np.where(dfForecast['R0']<=dfForecast['RL'], 1, 0)\n",
        "dfForecast['B1'] = np.where(dfForecast['R1']<=dfForecast['RL'], 1, 0)\n",
        "dfForecast['B2'] = np.where(dfForecast['R2']<=dfForecast['RL'], 1, 0)\n",
        "dfForecast['B3'] = np.where(dfForecast['R3']<=dfForecast['RL'], 1, 0)\n",
        "\n",
        "#Column that shows 1 if product enters basisutvalget, 0 if it stays where it is\n",
        "#and -1 if it leaves basisutvalget\n",
        "dfForecast['C1'] = dfForecast['B1'] - dfForecast['B0'] \n",
        "dfForecast['C2'] = dfForecast['B2'] - dfForecast['B0'] \n",
        "dfForecast['C3'] = dfForecast['B3'] - dfForecast['B0']\n",
        "\n",
        "#Takes in a series and returns ranking of values \n",
        "def ranking(x):\n",
        "  return (x.rank(method='min', ascending=False))\n",
        "  #return ss.rankdata(x)\n",
        "\n",
        "#Ranks forecasts for each date for t+1, t+2 and t+3\n",
        "dfForecast = dfForecast.assign(FR1=dfForecast.groupby(['År_Måned'])['F1'].apply(lambda x: ranking(x)))\n",
        "dfForecast = dfForecast.assign(FR2=dfForecast.groupby(['År_Måned'])['F2'].apply(lambda x: ranking(x)))\n",
        "dfForecast = dfForecast.assign(FR3=dfForecast.groupby(['År_Måned'])['F3'].apply(lambda x: ranking(x)))\n",
        "\n",
        "#Column that shows 1 if forecasted to be in basisutvalget(or below set limit) \n",
        "#and 0 otherwise\n",
        "dfForecast['FB1'] = np.where(dfForecast['FR1']<=dfForecast['RL'], 1, 0)\n",
        "dfForecast['FB2'] = np.where(dfForecast['FR2']<=dfForecast['RL'], 1, 0)\n",
        "dfForecast['FB3'] = np.where(dfForecast['FR3']<=dfForecast['RL'], 1, 0)\n",
        "\n",
        "#Column that shows 1 if product enters basisutvalget, 0 if it stays where it is\n",
        "#and -1 if it leaves basisutvalget, based on forecasts\n",
        "dfForecast['FC1'] = dfForecast['FB1'] - dfForecast['B0'] \n",
        "dfForecast['FC2'] = dfForecast['FB2'] - dfForecast['B0'] \n",
        "dfForecast['FC3'] = dfForecast['FB3'] - dfForecast['B0']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSdPQR6Ibg-r"
      },
      "source": [
        "#Evaluating forecasts for t+1, t+2 and t+3\n",
        "\n",
        "#Initializing lists for evaluation\n",
        "c1, fc1, c2, fc2, c3, fc3 = [],[],[],[],[],[]\n",
        "r0, r1, fr1, r2, fr2, r3, fr3 = [],[],[],[],[],[],[]\n",
        "dr1, dfr1, dr2, dfr2, dr3, dfr3 = [],[],[],[],[],[]\n",
        "\n",
        "#Ranking limit which adapts to date \n",
        "r_limit = styring(dfPG[eval_PG[0]])\n",
        "r_limit['År_Måned'] = pd.to_datetime(r_limit['År_Måned'])\n",
        "r_limit = r_limit.set_index(['År_Måned'])\n",
        "\n",
        "for date in test_dates:\n",
        "  #Ranking limit\n",
        "  if status=='full':\n",
        "    rl = int(r_limit.loc[date])\n",
        "    #Evaluating clases of products outside of basisutvalget as well\n",
        "    rlc = round(max(styring(dfPG[eval_PG[0]])['Styringstall'])*1.2)\n",
        "  #Using set ranking limit when running tests  \n",
        "  else:\n",
        "    rl = 10\n",
        "    rlc = 12\n",
        "  \n",
        "\n",
        "  #Classes used for precision and recall\n",
        "  c1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R1'])['C1'][:rlc])\n",
        "  fc1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R1'])['FC1'][:rlc])\n",
        "  c2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R2'])['C2'][:rlc])\n",
        "  fc2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R2'])['FC2'][:rlc])\n",
        "  c3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R3'])['C3'][:rlc])\n",
        "  fc3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R3'])['FC3'][:rlc])\n",
        "\n",
        "  #Rankings used for mean shift, ordered by current date's correct ranking\n",
        "  r0 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['R0'][:rl])\n",
        "  r1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['R1'][:rl])\n",
        "  fr1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['FR1'][:rl])\n",
        "  r2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['R2'][:rl])\n",
        "  fr2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['FR2'][:rl])\n",
        "  r3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['R3'][:rl])\n",
        "  fr3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R0'])['FR3'][:rl])\n",
        "\n",
        "  #Rankings used for mean difference ordered by evaluated date's correct ranking\n",
        "  dr1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R1'])['R1'][:rl])\n",
        "  dfr1 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R1'])['FR1'][:rl])\n",
        "  dr2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R2'])['R2'][:rl])\n",
        "  dfr2 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R2'])['FR2'][:rl])\n",
        "  dr3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R3'])['R3'][:rl])\n",
        "  dfr3 += list(dfForecast[dfForecast['År_Måned']==date].sort_values(['R3'])['FR3'][:rl])\n",
        "\n",
        "\n",
        "\n",
        "cm={}\n",
        "cm[1] = confusion_matrix(c1, fc1, labels=[1, 0, -1])\n",
        "cm[2] = confusion_matrix(c2, fc2, labels=[1, 0, -1])\n",
        "cm[3] = confusion_matrix(c3, fc3, labels=[1, 0, -1])\n",
        "\n",
        "#Calculating precision and recall\n",
        "\n",
        "pre = {}\n",
        "rec = {}\n",
        "\n",
        "pre[1] = precision_score(c1,fc1,average=None)\n",
        "rec[1] = recall_score(c1,fc1,average=None)\n",
        "pre[2] = precision_score(c2,fc2,average=None)\n",
        "rec[2] = recall_score(c2,fc2,average=None)\n",
        "pre[3] = precision_score(c3,fc3,average=None)\n",
        "rec[3] = recall_score(c3,fc3,average=None)\n",
        "\n",
        "#Calculating mean shift\n",
        "\n",
        "def mean_shift(ranks, f_ranks):\n",
        "  if len(ranks)==len(f_ranks):\n",
        "    difference = []\n",
        "    zip_object = zip(ranks, f_ranks)\n",
        "    for ranks, f_ranks in zip_object:\n",
        "      difference.append(np.abs(f_ranks-ranks))\n",
        "    difference= np.nan_to_num(difference)\n",
        "    mean = np.mean(difference)\n",
        "    return(mean)\n",
        "  else:\n",
        "    print('Wrong length of lists')\n",
        "\n",
        "shift = {}\n",
        "shift[4] = mean_shift(r0,r1)\n",
        "shift[1] = mean_shift(r0,fr1)\n",
        "shift[5] = mean_shift(r0,r2)/2\n",
        "shift[2] = mean_shift(r0,fr2)/2\n",
        "shift[6] = mean_shift(r0,r3)/3\n",
        "shift[3] = mean_shift(r0,fr3)/3\n",
        "\n",
        "#Calculating Spearmans rho\n",
        "rho={}\n",
        "rho[1],p = spearmanr(dr1,dfr1)\n",
        "rho[2],p = spearmanr(dr2,dfr2)\n",
        "rho[3],p = spearmanr(dr3,dfr3)\n",
        "\n",
        "#Ranking score based on difference\n",
        "def ranking_score(rank1, rank2, l):\n",
        "  n=len(rank1)\n",
        "  difference = []\n",
        "  zip_object = zip(rank1, rank2)\n",
        "  for ranks, f_ranks in zip_object:\n",
        "    difference.append(np.abs(f_ranks-ranks))\n",
        "  score = 1- sum([l if value >=l else value for value in difference])/(l*n)\n",
        "  return score\n",
        "\n",
        "score= {}\n",
        "l=10\n",
        "score[1]=ranking_score(dr1,dfr1,l)\n",
        "score[2]=ranking_score(dr2,dfr2,l)\n",
        "score[3]=ranking_score(dr3,dfr3,l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpw2EqNTFA93"
      },
      "source": [
        "#Results for the chosen model \n",
        "#Model choice is decided with the chosen_forecast variable at the beginning of\n",
        "#the Ranking section. \n",
        "\n",
        "for i in range(1,4):\n",
        "  print('Forecast t+',i,':')\n",
        "  print('Precision')\n",
        "  print(pre[i])\n",
        "  print('Recall')\n",
        "  print(rec[i])\n",
        "  print('Relative absolute shift')\n",
        "  print(shift[i]/shift[i+3])\n",
        "  print('Spearman\\'s rho')\n",
        "  print(rho[i])\n",
        "  print('Score')\n",
        "  print(score[i])\n",
        "  print('')\n",
        "evaluate_forecasts(test_actual_output, chosen_forecast, n_steps, n_output) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TltKvqoci-jD"
      },
      "source": [
        "#Results that can be copied directly into .csv file. Can fail when using \n",
        "#status = 'testing' as there are too few products to guarantee that a product \n",
        "#will leave or enter basisutvalget and precision and recall lack the indices we\n",
        "#request\n",
        "\n",
        "def find_mae(test, forecasts, n_lag, n_seq,i):\n",
        "  actual = [row[i] for row in test]\n",
        "  predicted = [forecast[i] for forecast in forecasts]\n",
        "  mae = mean_absolute_error(actual, predicted)\n",
        "  return mae\n",
        "\n",
        "results =[find_mae(test_actual_output, chosen_forecast, n_steps, n_output,0),\n",
        "          find_mae(test_actual_output, chosen_forecast, n_steps, n_output,1),\n",
        "          find_mae(test_actual_output, chosen_forecast, n_steps, n_output,2),\n",
        "          score[1],score[2],score[3],rho[1],rho[2],rho[3],shift[1]/shift[4],\n",
        "          shift[2]/shift[5],shift[3]/shift[6], pre[1][0],pre[1][1],pre[1][2],\n",
        "          pre[2][0],pre[2][1],pre[2][2],pre[3][0],pre[3][1],pre[3][2],rec[1][0],\n",
        "          rec[1][1],rec[1][2],rec[2][0],rec[2][1],rec[2][2], rec[3][0],\n",
        "          rec[3][1],rec[3][2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjsW2lAB7ivx"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxY13zcu-jSV"
      },
      "source": [
        "#Functions, dictionaries and lists neccessary for plotting\n",
        "# Use seaborn style defaults and set the default figure size\n",
        "sns.set(rc={'figure.figsize':(10, 6)})\n",
        "sns.set_palette(\"tab10\", n_colors=10, desat=0.5, color_codes=True)\n",
        "color=['b','r','g','y','m']\n",
        "\n",
        "#Returns only ranking limit and date of dataframe\n",
        "def styring(df):\n",
        "  styring = df[df['Rangering']==1]\n",
        "  return styring[['År_Måned', 'Styringstall']]\n",
        "\n",
        "#Returns only the ranking and date of given article in dataframe\n",
        "def rangering(df, article):\n",
        "  ranks = df[df['Artikkelnr']==article]\n",
        "  return ranks[['År_Måned', 'Rangering']]\n",
        "\n",
        "#Returns only the price and date (as index) of given article in dataframe\n",
        "def pricing(df, article):\n",
        "  price = df[df['Artikkelnr']==article]\n",
        "  price.set_index(pd.DatetimeIndex(price['År_Måned']), inplace=True)\n",
        "  return price['Segmentpris']\n",
        "\n",
        "#Identifying the top 5 wines jan 2007\n",
        "high100 = dfPG[100][dfPG[100]['Rangering']<=5]\n",
        "high100start = high100[high100['År_Måned']=='2007-01-01']\n",
        "highArticles100start=high100start['Artikkelnr'].tolist()\n",
        "highArticles100start=list(dict.fromkeys(highArticles100start))\n",
        "\n",
        "#Identifying the 5 lower ranked wines jan 2007\n",
        "low100 = dfPG[100][(dfPG[100]['Rangering']>50) & (dfPG[100]['Rangering']<=55)]\n",
        "low100start = low100[low100['År_Måned']=='2007-01-01']\n",
        "lowArticles100start=low100start['Artikkelnr'].tolist()\n",
        "lowArticles100start=list(dict.fromkeys(highArticles100start))\n",
        "\n",
        "#Identifying the top 5 wines sep 2019\n",
        "high100end = high100[high100['År_Måned']=='2019-09-01']\n",
        "highArticles100end=high100end['Artikkelnr'].tolist()\n",
        "highArticles100end=list(dict.fromkeys(highArticles100end))\n",
        "\n",
        "#Making a dictionary of the top 50 wines of each price group\n",
        "dfTop50={}\n",
        "for price in pg:\n",
        "  dfTop50[price] = dfPG[price][dfPG[price]['Rangering']<=50]\n",
        "\n",
        "#Making dictionary of lists of top 50 ranked wines jan 2007 for each price group\n",
        "Top50Articles={}\n",
        "for price in pg:\n",
        "  a = dfTop50[price][dfTop50[price]['År_Måned']=='2007-01-01']\n",
        "  a = a['Artikkelnr'].tolist()\n",
        "  a = list(dict.fromkeys(a))\n",
        "  Top50Articles[price] = a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxK8wq67Xp5E"
      },
      "source": [
        "plt.figure(dpi=150)\n",
        "plt.title('Sales vs Sales Category of Article 5518101',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Liters',size=14)\n",
        "for i in range(1):\n",
        "  plt.plot(dfRed2.loc[5518101]['Netto Salg'], color=color[i], label='Rolling Sum Top 60 ')\n",
        "  plt.plot(dfRed2.loc[5518101]['Liter'].rolling(6).sum(), color=color[1], label='Rolling Sum Total')\n",
        "  plt.plot(dfRed2.loc[5518101]['Liter'], color=color[4], label='Total')\n",
        "  #plt.plot(netto[i], color=color[i], ls='--')\n",
        "plt.axvspan(dates[33], dates[40], color=\"orange\", alpha=0.2)\n",
        "\n",
        "\n",
        "plt.axvspan(dates[72], dates[79], color=\"orange\", alpha=0.2)\n",
        "plt.axvspan(dates[79], dates[93], color=\"green\", alpha=0.2, label='Basisutvalget')\n",
        "plt.axvspan(dates[104], dates[110], color=\"orange\", alpha=0.2, label='Testutvalget')\n",
        "plt.axvspan(dates[110], dates[124], color=\"green\", alpha=0.2)\n",
        "plt.axvspan(dates[124], dates[125], color=\"grey\", alpha=0.1, label='Bestillingsutvalget')\n",
        "plt.axvspan(dates[125], dates[126], color=\"green\", alpha=0.2)\n",
        "plt.axvspan(dates[71], dates[72], color=\"red\", alpha=0.2, label='NaN')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"utvalg.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldayQ0PHemZi"
      },
      "source": [
        "#Plotting the ranking limits of the different price groups\n",
        "\n",
        "styringstall=np.zeros((7,153))\n",
        "\n",
        "#dates=styring(dfPG[100])['År_Måned']\n",
        "\n",
        "styringstall[0]=styring(dfPG[100])['Styringstall']\n",
        "styringstall[1]=styring(dfPG[125])['Styringstall']\n",
        "styringstall[2]=styring(dfPG[150])['Styringstall']\n",
        "styringstall[3]=styring(dfPG[175])['Styringstall']\n",
        "styringstall[4]=styring(dfPG[200])['Styringstall']\n",
        "styringstall[5]=styring(dfPG[250])['Styringstall']\n",
        "styringstall[6]=styring(dfPG[300])['Styringstall']\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.title('Ranking Limit of Red Wines in Base Selection per Price Group',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Ranking Limit',size=14)\n",
        "for i in range(7):\n",
        "  plt.plot(dates, styringstall[i],label=pricegroups[i])\n",
        "plt.axvline(datetime(2016, 3, 1))\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"styringstall.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RJ5cTSIHIeb"
      },
      "source": [
        "color=['b','r','g','y','m']\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.title('Time Series of Ranking of Top 5 Wines in Price Group 100-125 in January 2007',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Ranking',size=14)\n",
        "for i in range(len(highArticles100start)):\n",
        "  ts=rangering(dfPG[100], highArticles100start[i])\n",
        "  plt.plot(ts['År_Måned'], ts['Rangering'],  color=color[i], label=highArticles100start[i])\n",
        "for i in range(len(highArticles100start)):\n",
        "  ts=rangering(dfPG[125], highArticles100start[i])\n",
        "  plt.plot(ts['År_Måned'], ts['Rangering'], linestyle='--',  color=color[i])\n",
        "for i in range(len(highArticles100start)):\n",
        "  ts=rangering(dfPG[150], highArticles100start[i])\n",
        "  plt.plot(ts['År_Måned'], ts['Rangering'], linestyle=':',  color=color[i])\n",
        "\n",
        "plt.ylim(0,40)\n",
        "plt.gca().invert_yaxis()\n",
        "custom_lines = [Line2D([0], [0], color='black', linestyle='-'),\n",
        "                Line2D([0], [0], color='black', linestyle='--'),\n",
        "                Line2D([0], [0], color='black', linestyle=':')]\n",
        "\n",
        "\n",
        "legend1 = plt.legend(custom_lines, ['100-125', '125-150', '150-175'], title='Price Groups', loc='lower right')\n",
        "plt.legend(title='Article nr')\n",
        "pyplot.gca().add_artist(legend1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"first5_100.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfhCIf-zsmLH"
      },
      "source": [
        "plt.figure(dpi=150)\n",
        "plt.title('Time Series of Ranking of Top 5 Wines in Price Group 100-125 in September 2019',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Ranking',size=14)\n",
        "for i in range(len(highArticles100end)):\n",
        "  ts=rangering(dfPG[100], highArticles100end[i])\n",
        "  plt.plot(ts['År_Måned'], ts['Rangering'],  color=color[i], label=highArticles100end[i])\n",
        "for i in range(len(highArticles100end)):\n",
        "  ts=rangering(dfPG[99], highArticles100end[i])\n",
        "  plt.plot(ts['År_Måned'], ts['Rangering'], linestyle='--',  color=color[i])\n",
        "\n",
        "\n",
        "plt.ylim(0,40)\n",
        "plt.gca().invert_yaxis()\n",
        "custom_lines = [Line2D([0], [0], color='black', linestyle='-'),\n",
        "                Line2D([0], [0], color='black', linestyle='--'),\n",
        "                Line2D([0], [0], color='black', linestyle=':')]\n",
        "\n",
        "\n",
        "legend1 = plt.legend(custom_lines, ['100-125', '<100'], title='Price Groups', loc='lower right')\n",
        "plt.legend(title='Article nr')\n",
        "pyplot.gca().add_artist(legend1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"last5_100.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uPJ-NhFEeqB"
      },
      "source": [
        "plt.figure(dpi=150)\n",
        "plt.title('Six Month Rolling Sum vs Selection of Article 5518101',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Liters',size=14)\n",
        "for i in range(1):\n",
        "  plt.plot(dfRed2.loc[5518101]['Netto Salg'], color=color[i], label='Sales Top 60 Stores')\n",
        "  plt.plot(dfRed2.loc[5518101]['Liter'].rolling(6).sum(), color=color[1], label='Sales Total')\n",
        "  #plt.plot(netto[i], color=color[i], ls='--')\n",
        "plt.axvspan(dates[33], dates[40], color=\"orange\", alpha=0.2)\n",
        "\n",
        "\n",
        "plt.axvspan(dates[72], dates[79], color=\"orange\", alpha=0.2)\n",
        "plt.axvspan(dates[79], dates[93], color=\"green\", alpha=0.2, label='Basisutvalget')\n",
        "plt.axvspan(dates[104], dates[110], color=\"orange\", alpha=0.2, label='Testutvalget')\n",
        "plt.axvspan(dates[110], dates[124], color=\"green\", alpha=0.2)\n",
        "plt.axvspan(dates[124], dates[125], color=\"grey\", alpha=0.1, label='Bestillingsutvalget')\n",
        "plt.axvspan(dates[125], dates[126], color=\"green\", alpha=0.2)\n",
        "plt.axvspan(dates[71], dates[72], color=\"red\", alpha=0.2, label='NaN')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"utvalg.png\")#Finding median price of the top 50 articles for the different price groups as \n",
        "#they evolve over time\n",
        "\n",
        "avgPrices={}\n",
        "\n",
        "#Looping over price groups\n",
        "for pricegroup in pg:\n",
        "\n",
        "  #Setting up dataframe with random article\n",
        "  dfPrices=pricing(dfRed,755501)\n",
        "\n",
        "  #Looping over articles \n",
        "  for article in Top50Articles[pricegroup]:\n",
        "    price = pricing(dfRed, article)\n",
        "    dfPrices = pd.concat([dfPrices, price], axis=1)\n",
        "\n",
        "  #Naming columns to remove initial article\n",
        "  dfPrices.columns = [755501]+ Top50Articles[pricegroup]\n",
        "  dfPrices.pop(755501)\n",
        "\n",
        "  avgPrices[pricegroup] = dfPrices.mean(axis=1)\n",
        "\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.title('Time Series of Average Price of Top 50 Wines per Price Group in January 2007',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Price',size=14)\n",
        "for i in range(7):\n",
        "  plt.plot(dates, avgPrices[pg[i+1]],label=pricegroups[i])\n",
        "plt.axhline(100, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(125, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(150, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(175, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(200, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(250, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(300, ls='--', lw=0.5, color='black')\n",
        "plt.axhline(400, ls='--', lw=0.5, color='black')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"priceEvolution.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJHtA6HPsOpz"
      },
      "source": [
        "liter = np.zeros((5,len(dates)))\n",
        "netto = np.zeros((5,len(dates)))\n",
        "for i in range(len(highArticles100start)):\n",
        "  for j in range(len(dates)):\n",
        "    liter[i,j]=dfRed2.loc[lowArticles100start[i],dates[j]]['Liter']#!!!!!!!!!!!!!!!!\n",
        "    netto[i,j]=dfRed2.loc[lowArticles100start[i],dates[j]]['Netto Salg']#!!!!!!!!!!!!\n",
        "\n",
        "litersAgg = pd.DataFrame(np.transpose(liter)).rolling(6).sum()\n",
        "netto = pd.DataFrame(np.transpose(netto))\n",
        "\n",
        "litersAgg.index = dates\n",
        "netto.index = dates\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.title('Six Month Rolling Sum of Top 5 Wines in Price Group 100-125 in January 2007',size=14)\n",
        "plt.xlabel('Years',size=14)\n",
        "plt.ylabel('Liters',size=14)\n",
        "for i in range(len(highArticles100start)):\n",
        "  plt.plot(litersAgg[i], color=color[i], label=highArticles100start[i])\n",
        "  plt.plot(netto[i], color=color[i], ls='--')\n",
        "plt.legend()\n",
        "custom_lines = [Line2D([0], [0], color='black', linestyle='-'),\n",
        "                Line2D([0], [0], color='black', linestyle='--')]\n",
        "\n",
        "\n",
        "legend1 = plt.legend(custom_lines, ['Total', 'Top 60 stores'], title='Sales', loc='upper left')\n",
        "plt.legend()\n",
        "pyplot.gca().add_artist(legend1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"nettoVStotal.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnP3VjMUTrf5"
      },
      "source": [
        "liters_chg = pd.DataFrame()\n",
        "netto_chg = pd.DataFrame()\n",
        "\n",
        "for i in range(len(highArticles100start)):\n",
        "  liters_chg[i] = litersAgg[i].pct_change()\n",
        "  netto_chg[i] = netto[i].pct_change()\n",
        "\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.title('Scatter Plot of Sales Top 5 Wines in Price Group 100-125 in January 2007',size=14)\n",
        "plt.xlabel('Percent Change (Total)',size=14)\n",
        "plt.ylabel('Percent Change (Top 60 Stores)',size=14)\n",
        "for i in range(5):\n",
        "  plt.scatter(liters_chg[i], netto_chg[i], color=color[i], label=highArticles100start[i])\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"scatterplot.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}