{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WinePreprocessing.ipynb",
      "provenance": [],
      "mount_file_id": "1EpdW3FthedhVDcHW86shTdr2b0TOx2rK",
      "authorship_tag": "ABX9TyMxzcV3+VxsfHZ0hePv1OCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteffiJF/ForecastingRedWineRankings/blob/main/WinePreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjCvVK-FSBxs"
      },
      "source": [
        "Merging the data from sales, ranking, and products, and making two files, one for red wine with less than 75 g sugar and one for redwine with less than 9 g sugar. These two files are concatenated in WineMaster.ipynb. Most of the code in this file was produced for my specialization project and is reused here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uXzftsEhkNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75343e4-3552-45a9-d36c-d428abb462d6"
      },
      "source": [
        "!pip install \"dask[dataframe]\" \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import dask.dataframe as dd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "from numpy import array\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.19.5)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.1.5)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.1)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-2021.7.0 locket-0.2.1 partd-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpSG_kXn8Sr5"
      },
      "source": [
        "Downloading data from Google Disk and merging the files (you need to upload the files into your own Google Disk and change the path to use)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69x2X38l6YnY"
      },
      "source": [
        "#Downloading sales data\n",
        "sales = pd.read_csv('/content/drive/MyDrive/vindata/Salgsstatistikk_for_dbe_import_2019_11_30.csv', sep=';',  parse_dates = [['År', 'Måned']], low_memory=False)\n",
        "#Downloading ranking data\n",
        "ranks = pd.read_csv('/content/drive/MyDrive/vindata/Rangeringsrapport_for_dbe_import_2019_11_30.csv', sep=';',  parse_dates = [['År', 'Måned']], low_memory=False)\n",
        "#Downloading product data\n",
        "products = pd.read_csv('/content/drive/MyDrive/vindata/Products_for_database_import_2019_11_30.csv',sep=';',error_bad_lines=False)\n",
        "\n",
        "#Splitting dataset into seperate product types \n",
        "redSales=sales[sales['Varetype']==\"Rødvin\"]\n",
        "whiteSales=sales[sales['Varetype']==\"Hvitvin\"]\n",
        "redRanks = ranks[ranks['Produktgruppe']=='Rødvin < 9 g sukker per liter']\n",
        "\n",
        "#Changing column names to match data frames\n",
        "products = products.rename(columns={\"VMP ID\": \"Artikkelnr\", \"Alkohol %\": \"Alkoholprosent\"})\n",
        "\n",
        "#Keeping interesting columns\n",
        "smallRedSales=redSales[['År_Måned','Artikkelnr','Liter denne måned i år', 'Salgspris', 'Land', 'Distrikt', 'Årgang', 'Volum', 'Alkoholprosent', 'Utvalg',]]\n",
        "smallRanks=redRanks[['År_Måned','Artikkelnr', 'Rangering', 'Status',\t'Styringstall', 'Netto Salg', 'Produktgruppe', 'Segmentpris', 'Fredet','Minimum', 'Maksimum']]\n",
        "smallProducts=products[['Artikkelnr', 'Årgang', 'Alkoholprosent', 'Emballasjetype']]\n",
        "\n",
        "#Converting to dask dataframe for merging\n",
        "dfRedSales = dd.from_pandas(smallRedSales, npartitions=10)\n",
        "dfRanks = dd.from_pandas(smallRanks, npartitions=10)\n",
        "dfProducts = dd.from_pandas(smallProducts, npartitions=10)\n",
        "\n",
        "# Merge the csv files. Changing order of ranks and sales compared to last project.\n",
        "df = dd.merge(dfRanks, dfRedSales, how='left', on=['Artikkelnr', 'År_Måned'])\n",
        "df = dd.merge(df, dfProducts, how='left', on=['Artikkelnr','Årgang'])\n",
        "\n",
        "#Converting back to pandas\n",
        "df=df.compute()\n",
        "\n",
        "#Sorting data\n",
        "df=df.sort_values(by=['År_Måned','Land', 'Artikkelnr', 'Årgang', 'Distrikt'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh4ADmojONHM"
      },
      "source": [
        "Adding relevant columns and removing irrelevant rows and NaN values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZziQf9CTHvcw"
      },
      "source": [
        "#Summing up total amount of wine sold per article nr and adding as column\n",
        "totalLiters=df.groupby(['Artikkelnr'])['Liter denne måned i år'].sum()\n",
        "totalLiters=totalLiters.to_dict()\n",
        "df['Artikkelnr']\n",
        "df['Liter totalt']= df['Artikkelnr'].map(totalLiters)\n",
        "\n",
        "#Using Alkoholprosent_x if not NaN, otherwise using Alkoholprosent_y\n",
        "df['Alkoholprosent'] = np.where(df['Alkoholprosent_x'].notnull(), df['Alkoholprosent_x'], df['Alkoholprosent_y'])\n",
        "df=df.drop(['Alkoholprosent_x','Alkoholprosent_y'], axis='columns')\n",
        "\n",
        "#Adding a column for amount of alcohol divided by sales price\n",
        "df['Alk/Pris'] = df['Alkoholprosent']*df['Volum']/df['Salgspris']\n",
        "\n",
        "#Adding a column for volume divided by sales price\n",
        "df['Vol/Pris'] = df['Volum']/df['Salgspris']\n",
        "\n",
        "#Changing name of column to avoid space\n",
        "df = df.rename(columns={'Liter denne måned i år': 'Liter'})\n",
        "\n",
        "#Removes true duplicates \n",
        "df=df.drop_duplicates()\n",
        "\n",
        "#Removing infinite values\n",
        "df=df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "#Making a copy of the dataframe with multiindex\n",
        "df2 = df.set_index(['Artikkelnr','År_Måned']).sort_index()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryHSc-_Tq-A1"
      },
      "source": [
        "Removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUOQBnbaq_0c"
      },
      "source": [
        "#Handling duplicates\n",
        "\n",
        "#Finding dates that are duplicated in list of dates and returning\n",
        "#a string that can be used as index for dataframe\n",
        "def dupDates(dates):\n",
        "  seen = {}\n",
        "  dupes = []\n",
        "\n",
        "  for i in dates:\n",
        "      if i not in seen:\n",
        "          seen[i] = 1\n",
        "      else:\n",
        "          if seen[i] == 1:\n",
        "              dupes.append(i)\n",
        "          seen[i] += 1\n",
        "  for i in range(0,len(dupes)):\n",
        "    dupes[i]=str(dupes[i])\n",
        "    dupes[i]=dupes[i].replace('-01T00:00:00.000000000','')\n",
        "  return dupes\n",
        "\n",
        "#Finds the first duplicate and returns article number and date.\n",
        "#Helps initiate new dataframe containing only duplicated rows\n",
        "def arbitrary_duplicate(df2, articles):\n",
        "  if articles:\n",
        "    for i in articles:\n",
        "      dates=df2.loc[i].index.values\n",
        "      if len(dates)-len(np.unique(dates))!=0:\n",
        "        dupes=dupDates(dates)\n",
        "        return i, dupes[0]\n",
        "  return 0, 0\n",
        "\n",
        "\n",
        "#Finding articlenrs with duplicates and returning a tuple\n",
        "#with duplicated article number and date\n",
        "def duplicate_index(df2,dfDupes, articles):\n",
        "  dupRows= []\n",
        "  for i in articles:\n",
        "    dates=df2.loc[i].index.values\n",
        "    if len(dates)-len(np.unique(dates))!=0:\n",
        "      dupes=dupDates(dates)\n",
        "      for j in range(0,len(dupes)):\n",
        "        dfDupes=dfDupes.append(df2.loc[i][dupes[j]])\n",
        "        d=(i,dupes[j])\n",
        "        dupRows.append(d)\n",
        "  return dupRows \n",
        "\n",
        "\n",
        "#Identifying the columns that vary in the duplicated rows\n",
        "def nonduplicated_columns(df):\n",
        "    my_cols = []\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique(dropna=False) > 1:\n",
        "            my_cols.append(list(df.columns).index(col))\n",
        "    return my_cols\n",
        "\n",
        "#Printing the columns and amount of times that column is the\n",
        "#reason for duplicated rows\n",
        "def duplicates():\n",
        "  duplicatedColumns=np.zeros(len(df2.columns))\n",
        "\n",
        "  for art,date in dupRows:\n",
        "    #print(art, date)\n",
        "    dupCols=nonduplicated_columns(df2.loc[art][date])\n",
        "    for i in range(0,len(dupCols)):\n",
        "      duplicatedColumns[dupCols[i]]+=1\n",
        "\n",
        "  for i in range(0,len(duplicatedColumns)):\n",
        "    print(df2.columns[i],duplicatedColumns[i])\n",
        "\n",
        "#Duplicated rows caused by different amounts of liter sold per month are often \n",
        "#caused by multiple distributors, these rows will be handled by adding the Liter \n",
        "#columns to one row and removing the rest of the duplicated rows\n",
        "def remove_duplicates(df, df2, dupRows):\n",
        "  for art,date in dupRows:\n",
        "      dupCols=nonduplicated_columns(df2.loc[art][date])\n",
        "      indices=df.index[(df['Artikkelnr'] == art) & (df['År_Måned'] == date)].tolist()\n",
        "      if 8 in dupCols:\n",
        "        print(indices)\n",
        "        df.loc[indices[0],'Liter']+=df.loc[indices[1]][2]\n",
        "        df=df.drop(indices[1])\n",
        "      else:\n",
        "        for i in range(1,len(indices)):\n",
        "          df=df.drop(indices[i])\n",
        "  return df\n",
        "\n",
        "\n",
        "#List of article numbers\n",
        "articles=df['Artikkelnr'].tolist()\n",
        "articles=list(dict.fromkeys(articles))\n",
        "\n",
        "#Resetting index to remove problem of rows having same index\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "#Setting up new dataframe for duplicated rows, starting with arbitrary duplicate\n",
        "dart, ddate = arbitrary_duplicate(df2, articles)\n",
        "\n",
        "if dart != 0:\n",
        "  dfDupes=df2.loc[dart][ddate]\n",
        "  \n",
        "  #Tuples with duplicate articles and dates  \n",
        "  dupRows=duplicate_index(df2,dfDupes, articles)\n",
        "  \n",
        "  #Removing duplicates\n",
        "  df= remove_duplicates(df, df2, dupRows)\n",
        "\n",
        "#Making a copy of the dataframe with multiindex\n",
        "df2 = df.set_index(['Artikkelnr','År_Måned']).sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOsHxsESD0_m"
      },
      "source": [
        "df.to_csv('RedWines9.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}